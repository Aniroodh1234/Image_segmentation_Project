{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport random\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms\n\nprint(f\"Python: {sys.version.split()[0]}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Torchvision: {torchvision.__version__}\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_voc_dataset():\n    \"\"\"Automatically find VOC dataset path in Kaggle input directory\"\"\"\n    input_base = Path('/kaggle/input')\n    \n    # Based on your screenshot, the structure is:\n    # pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/\n    potential_paths = [\n        input_base / 'pascal-voc-2012-dataset' / 'VOC2012_train_val' / 'VOC2012_train_val',\n        input_base / 'pascal-voc-2012-dataset' / 'VOC2012_test' / 'VOC2012_test',\n        input_base / 'pascal-voc-2012-dataset' / 'VOCdevkit' / 'VOC2012',\n        input_base / 'pascal-voc-2012-dataset' / 'VOC2012',\n        input_base / 'pascal-voc-2012-dataset',\n    ]\n    \n    for p in potential_paths:\n        if p.exists() and (p / 'JPEGImages').exists():\n            print(f\"✓ Found dataset at: {p}\")\n            return p\n    \n    # Fallback: search recursively\n    print(\"Searching for JPEGImages directory...\")\n    for root, dirs, files in os.walk(input_base):\n        if 'JPEGImages' in dirs and 'SegmentationClass' in dirs:\n            found_path = Path(root)\n            print(f\"✓ Found dataset at: {found_path}\")\n            return found_path\n    \n    raise FileNotFoundError(\"Could not find VOC dataset. Please check the input directory.\")\n\nROOT = find_voc_dataset()\nprint(f\"\\nDataset root: {ROOT}\")\nprint(f\"\\nDataset structure:\")\nfor item in sorted(ROOT.iterdir()):\n    if item.is_dir():\n        file_count = len(list(item.glob('*')))\n        print(f\"  📁 {item.name}/ ({file_count} items)\")\n    else:\n        print(f\"  📄 {item.name}\")\n\n# Verify critical directories\nassert (ROOT / 'JPEGImages').exists(), \"JPEGImages directory not found\"\nassert (ROOT / 'SegmentationClass').exists(), \"SegmentationClass directory not found\"\n\njpg_count = len(list((ROOT / 'JPEGImages').glob('*.jpg')))\npng_count = len(list((ROOT / 'SegmentationClass').glob('*.png')))\nprint(f\"\\n✓ Found {jpg_count} images and {png_count} masks\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_pascal_label_colormap():\n    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\"\"\"\n    colormap = np.zeros((256, 3), dtype=np.uint8)\n    \n    for i in range(256):\n        r = g = b = 0\n        cid = i\n        for j in range(8):\n            r |= ((cid >> 0) & 1) << (7 - j)\n            g |= ((cid >> 1) & 1) << (7 - j)\n            b |= ((cid >> 2) & 1) << (7 - j)\n            cid >>= 3\n        colormap[i] = [r, g, b]\n    \n    return colormap\n\nVOC_COLORMAP = create_pascal_label_colormap()\nVOC_CLASSES = [\n    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n    'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n    'sofa', 'train', 'tvmonitor'\n]\n\ndef mask_to_class_index(mask_pil):\n    \"\"\"Convert RGB mask to class indices (0-20) with 255 for ignore.\"\"\"\n    arr = np.array(mask_pil.convert('RGB'), dtype=np.uint8)\n    h, w, _ = arr.shape\n    \n    # Create mapping from color to class index (only 21 classes used)\n    color_to_idx = {tuple(VOC_COLORMAP[i]): i for i in range(21)}\n    \n    # Initialize with ignore index\n    result = np.full((h, w), 255, dtype=np.uint8)\n    \n    # Map each pixel\n    arr_flat = arr.reshape(-1, 3)\n    result_flat = np.full(arr_flat.shape[0], 255, dtype=np.uint8)\n    \n    for color, idx in color_to_idx.items():\n        matches = np.all(arr_flat == color, axis=1)\n        result_flat[matches] = idx\n    \n    return result_flat.reshape(h, w)\n\ndef class_index_to_rgb(mask_arr):\n    \"\"\"Convert class indices back to RGB for visualization.\"\"\"\n    h, w = mask_arr.shape\n    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    for cls_idx in range(21):\n        rgb[mask_arr == cls_idx] = VOC_COLORMAP[cls_idx]\n    \n    return rgb\n\nprint(f\"✓ VOC colormap created with {len(VOC_CLASSES)} classes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(Conv => BN => ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=21, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\nprint(\"✓ U-Net architecture defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PascalVOCDataset(Dataset):\n    \"\"\"Pascal VOC 2012 Segmentation Dataset\"\"\"\n    \n    def __init__(self, root, image_ids, img_size=(256, 256), augment=False):\n        self.root = Path(root)\n        self.image_ids = image_ids\n        self.img_size = img_size\n        self.augment = augment\n        \n        self.img_dir = self.root / 'JPEGImages'\n        self.mask_dir = self.root / 'SegmentationClass'\n        \n        # Data augmentation transforms\n        self.color_jitter = transforms.ColorJitter(\n            brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1\n        )\n    \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        \n        # Load image and mask\n        img_path = self.img_dir / f\"{img_id}.jpg\"\n        mask_path = self.mask_dir / f\"{img_id}.png\"\n        \n        if not img_path.exists() or not mask_path.exists():\n            # Return next valid item if this one is missing\n            return self.__getitem__((idx + 1) % len(self))\n        \n        img = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path)\n        \n        # Resize\n        img = img.resize(self.img_size, Image.BILINEAR)\n        mask = mask.resize(self.img_size, Image.NEAREST)\n        \n        # Apply augmentations\n        if self.augment:\n            # Random horizontal flip\n            if random.random() > 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n            \n            # Random vertical flip\n            if random.random() > 0.5:\n                img = img.transpose(Image.FLIP_TOP_BOTTOM)\n                mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n            \n            # Color jittering (only on image)\n            img = self.color_jitter(img)\n        \n        # Convert mask to class indices\n        mask_arr = mask_to_class_index(mask)\n        \n        # Convert to tensors\n        img_tensor = transforms.ToTensor()(img)\n        img_tensor = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )(img_tensor)\n        \n        mask_tensor = torch.from_numpy(mask_arr).long()\n        \n        return img_tensor, mask_tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all valid image IDs (those with both image and mask)\nall_img_ids = [p.stem for p in sorted((ROOT / 'JPEGImages').glob('*.jpg'))]\n\nvalid_ids = []\nfor img_id in all_img_ids:\n    img_path = ROOT / 'JPEGImages' / f'{img_id}.jpg'\n    mask_path = ROOT / 'SegmentationClass' / f'{img_id}.png'\n    if img_path.exists() and mask_path.exists():\n        valid_ids.append(img_id)\n\nprint(f\"Total valid samples: {len(valid_ids)}\")\n\n# Split into train and validation (85/15 split)\ntrain_ids, val_ids = train_test_split(\n    valid_ids, test_size=0.15, random_state=SEED\n)\n\nprint(f\"Train samples: {len(train_ids)}\")\nprint(f\"Val samples: {len(val_ids)}\")\n\n# Create datasets\nIMG_SIZE = (256, 256)  # U-Net typically uses 256x256 or 512x512\n\ntrain_dataset = PascalVOCDataset(\n    ROOT, train_ids, img_size=IMG_SIZE, augment=True\n)\nval_dataset = PascalVOCDataset(\n    ROOT, val_ids, img_size=IMG_SIZE, augment=False\n)\n\n# Create dataloaders\nBATCH_SIZE = 8  # Adjust based on GPU memory\nNUM_WORKERS = 2\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    drop_last=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"\\n✓ DataLoaders created\")\nprint(f\"  Image size: {IMG_SIZE}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize(img_tensor):\n    \"\"\"Denormalize image tensor for visualization\"\"\"\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    return img_tensor * std + mean\n\ndef visualize_batch(images, masks, predictions=None, num_samples=4):\n    \"\"\"Visualize a batch of images with masks and predictions\"\"\"\n    num_samples = min(num_samples, images.shape[0])\n    \n    cols = 3 if predictions is not None else 2\n    fig, axes = plt.subplots(num_samples, cols, figsize=(cols*5, num_samples*5))\n    \n    if num_samples == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i in range(num_samples):\n        # Denormalize and convert image\n        img = denormalize(images[i]).cpu().permute(1, 2, 0).numpy()\n        img = np.clip(img, 0, 1)\n        \n        # Convert mask to RGB\n        mask = masks[i].cpu().numpy()\n        mask_rgb = class_index_to_rgb(mask)\n        \n        # Plot image and mask\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title('Image', fontsize=14, fontweight='bold')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(mask_rgb)\n        axes[i, 1].set_title('Ground Truth', fontsize=14, fontweight='bold')\n        axes[i, 1].axis('off')\n        \n        # Plot prediction if available\n        if predictions is not None:\n            pred = predictions[i].cpu().numpy()\n            pred_rgb = class_index_to_rgb(pred)\n            axes[i, 2].imshow(pred_rgb)\n            axes[i, 2].set_title('Prediction', fontsize=14, fontweight='bold')\n            axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Show sample batch\nsample_images, sample_masks = next(iter(train_loader))\nprint(f\"Sample batch shape: images={sample_images.shape}, masks={sample_masks.shape}\")\nvisualize_batch(sample_images, sample_masks, num_samples=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_CLASSES = 21\n\n# Initialize U-Net model\nmodel = UNet(n_channels=3, n_classes=NUM_CLASSES, bilinear=True)\nmodel = model.to(device)\n\n# Loss function (ignore index 255)\ncriterion = nn.CrossEntropyLoss(ignore_index=255)\n\n# Optimizer - Adam with weight decay\noptimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-7\n)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"U-Net Model Initialized\")\nprint(f\"{'='*60}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(pred, target, num_classes=21, ignore_index=255):\n    \"\"\"Compute Intersection over Union (IoU) for each class\"\"\"\n    pred = pred.flatten()\n    target = target.flatten()\n    \n    # Remove ignore index\n    mask = target != ignore_index\n    pred = pred[mask]\n    target = target[mask]\n    \n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred == cls)\n        target_cls = (target == cls)\n        \n        intersection = (pred_cls & target_cls).sum().item()\n        union = (pred_cls | target_cls).sum().item()\n        \n        if union == 0:\n            ious.append(float('nan'))  # Class not present\n        else:\n            ious.append(intersection / union)\n    \n    return ious\n\ndef compute_pixel_accuracy(pred, target, ignore_index=255):\n    \"\"\"Compute pixel-wise accuracy\"\"\"\n    mask = target != ignore_index\n    correct = (pred[mask] == target[mask]).sum().item()\n    total = mask.sum().item()\n    return correct / total if total > 0 else 0.0\n\ndef compute_dice_score(pred, target, num_classes=21, ignore_index=255):\n    \"\"\"Compute Dice coefficient for each class\"\"\"\n    pred = pred.flatten()\n    target = target.flatten()\n    \n    # Remove ignore index\n    mask = target != ignore_index\n    pred = pred[mask]\n    target = target[mask]\n    \n    dice_scores = []\n    for cls in range(num_classes):\n        pred_cls = (pred == cls)\n        target_cls = (target == cls)\n        \n        intersection = (pred_cls & target_cls).sum().item()\n        pred_sum = pred_cls.sum().item()\n        target_sum = target_cls.sum().item()\n        \n        if pred_sum + target_sum == 0:\n            dice_scores.append(float('nan'))\n        else:\n            dice_scores.append(2 * intersection / (pred_sum + target_sum))\n    \n    return dice_scores\n\nprint(\"✓ Metrics functions defined (IoU, Pixel Accuracy, Dice Score)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device, epoch):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    \n    running_loss = 0.0\n    running_acc = 0.0\n    num_batches = len(loader)\n    \n    for batch_idx, (images, masks) in enumerate(loader):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Metrics\n        preds = outputs.argmax(dim=1)\n        acc = compute_pixel_accuracy(preds, masks)\n        \n        running_loss += loss.item()\n        running_acc += acc\n        \n        # Print progress\n        if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == num_batches:\n            print(f\"  [{batch_idx+1:3d}/{num_batches}] \"\n                  f\"Loss: {loss.item():.4f} | Acc: {acc:.4f}\")\n    \n    epoch_loss = running_loss / num_batches\n    epoch_acc = running_acc / num_batches\n    \n    return epoch_loss, epoch_acc\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    \n    running_loss = 0.0\n    running_acc = 0.0\n    all_ious = []\n    all_dice = []\n    \n    with torch.no_grad():\n        for images, masks in loader:\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            # Metrics\n            preds = outputs.argmax(dim=1)\n            acc = compute_pixel_accuracy(preds, masks)\n            \n            # Compute IoU and Dice for each sample in batch\n            for pred, mask in zip(preds, masks):\n                ious = compute_iou(pred.cpu().numpy(), mask.cpu().numpy())\n                dice = compute_dice_score(pred.cpu().numpy(), mask.cpu().numpy())\n                all_ious.append(ious)\n                all_dice.append(dice)\n            \n            running_loss += loss.item()\n            running_acc += acc\n    \n    epoch_loss = running_loss / len(loader)\n    epoch_acc = running_acc / len(loader)\n    \n    # Compute mean IoU and Dice (excluding NaN values)\n    all_ious = np.array(all_ious)\n    all_dice = np.array(all_dice)\n    mean_ious = np.nanmean(all_ious, axis=0)\n    mean_dice = np.nanmean(all_dice, axis=0)\n    mean_iou = np.nanmean(mean_ious)\n    mean_dice_score = np.nanmean(mean_dice)\n    \n    return epoch_loss, epoch_acc, mean_iou, mean_dice_score, mean_ious\n\nprint(\"✓ Training and validation functions defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EPOCHS = 30\nbest_miou = 0.0\n\nhistory = {\n    'train_loss': [], 'train_acc': [],\n    'val_loss': [], 'val_acc': [], \n    'val_miou': [], 'val_dice': []\n}\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Starting U-Net Training for {NUM_EPOCHS} epochs\")\nprint(f\"{'='*70}\\n\")\n\nstart_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_start = time.time()\n    \n    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n    print(\"-\" * 70)\n    \n    # Train\n    train_loss, train_acc = train_one_epoch(\n        model, train_loader, criterion, optimizer, device, epoch\n    )\n    \n    # Validate\n    val_loss, val_acc, val_miou, val_dice, class_ious = validate(\n        model, val_loader, criterion, device\n    )\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n    \n    # Save history\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_miou'].append(val_miou)\n    history['val_dice'].append(val_dice)\n    \n    # Print epoch summary\n    epoch_time = time.time() - epoch_start\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1} Summary:\")\n    print(f\"  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n    print(f\"  Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n    print(f\"         mIoU: {val_miou:.4f} | Dice: {val_dice:.4f}\")\n    print(f\"  Time: {epoch_time:.2f}s | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n    \n    # Save best model\n    if val_miou > best_miou:\n        best_miou = val_miou\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_miou': val_miou,\n            'val_dice': val_dice,\n        }, '/kaggle/working/best_unet_model.pth')\n        print(f\"  ✓ Saved best model! (mIoU: {best_miou:.4f})\")\n    \n    print(f\"{'='*70}\")\n\ntotal_time = time.time() - start_time\nprint(f\"\\n{'='*70}\")\nprint(f\"Training Completed!\")\nprint(f\"{'='*70}\")\nprint(f\"Total time: {total_time/60:.2f} minutes\")\nprint(f\"Best validation mIoU: {best_miou:.4f}\")\nprint(f\"{'='*70}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Loss plot\naxes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\naxes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\naxes[0, 0].set_xlabel('Epoch', fontsize=12)\naxes[0, 0].set_ylabel('Loss', fontsize=12)\naxes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0, 0].legend(fontsize=11)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Accuracy plot\naxes[0, 1].plot(history['train_acc'], label='Train Acc', marker='o', linewidth=2, color='green')\naxes[0, 1].plot(history['val_acc'], label='Val Acc', marker='s', linewidth=2, color='orange')\naxes[0, 1].set_xlabel('Epoch', fontsize=12)\naxes[0, 1].set_ylabel('Accuracy', fontsize=12)\naxes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\naxes[0, 1].legend(fontsize=11)\naxes[0, 1].grid(True, alpha=0.3)\n\n# mIoU plot\naxes[1, 0].plot(history['val_miou'], label='Val mIoU', marker='s', linewidth=2, color='purple')\naxes[1, 0].set_xlabel('Epoch', fontsize=12)\naxes[1, 0].set_ylabel('mIoU', fontsize=12)\naxes[1, 0].set_title('Validation Mean IoU', fontsize=14, fontweight='bold')\naxes[1, 0].legend(fontsize=11)\naxes[1, 0].grid(True, alpha=0.3)\n\n# Dice score plot\naxes[1, 1].plot(history['val_dice'], label='Val Dice', marker='s', linewidth=2, color='red')\naxes[1, 1].set_xlabel('Epoch', fontsize=12)\naxes[1, 1].set_ylabel('Dice Score', fontsize=12)\naxes[1, 1].set_title('Validation Dice Score', fontsize=14, fontweight='bold')\naxes[1, 1].legend(fontsize=11)\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/unet_training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Training history plots saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"Per-Class Performance Analysis on Validation Set\")\nprint(\"=\"*70)\n\n# Compute per-class metrics on full validation set\nmodel.eval()\nall_class_ious = []\nall_class_dice = []\n\nwith torch.no_grad():\n    for images, masks in val_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n        \n        for pred, mask in zip(preds, masks):\n            ious = compute_iou(pred.cpu().numpy(), mask.cpu().numpy())\n            dice = compute_dice_score(pred.cpu().numpy(), mask.cpu().numpy())\n            all_class_ious.append(ious)\n            all_class_dice.append(dice)\n\n# Calculate mean per class\nall_class_ious = np.array(all_class_ious)\nall_class_dice = np.array(all_class_dice)\nmean_ious = np.nanmean(all_class_ious, axis=0)\nmean_dice = np.nanmean(all_class_dice, axis=0)\n\n# Create results DataFrame\nresults = []\nfor idx, (cls_name, iou, dice) in enumerate(zip(VOC_CLASSES, mean_ious, mean_dice)):\n    if not np.isnan(iou):\n        results.append({\n            'Class': cls_name,\n            'IoU': f\"{iou:.4f}\",\n            'Dice': f\"{dice:.4f}\",\n            'IoU_numeric': iou,\n            'Dice_numeric': dice\n        })\n\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('IoU_numeric', ascending=False)\nresults_df_display = results_df[['Class', 'IoU', 'Dice']].reset_index(drop=True)\n\nprint(\"\\n\" + results_df_display.to_string(index=False))\nprint(\"\\n\" + \"=\"*70)\nprint(f\"Overall Mean IoU: {np.nanmean(mean_ious):.4f}\")\nprint(f\"Overall Mean Dice: {np.nanmean(mean_dice):.4f}\")\nprint(\"=\"*70)\n\n# Save results\nresults_df_display.to_csv('/kaggle/working/unet_class_metrics.csv', index=False)\nprint(\"\\n✓ Per-class metrics saved to /kaggle/working/unet_class_metrics.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot per-class IoU\nfig, ax = plt.subplots(figsize=(14, 8))\n\nclasses_with_data = [results_df.iloc[i]['Class'] for i in range(len(results_df))]\nious_with_data = [results_df.iloc[i]['IoU_numeric'] for i in range(len(results_df))]\n\ncolors = plt.cm.viridis(np.linspace(0, 1, len(classes_with_data)))\nbars = ax.barh(classes_with_data, ious_with_data, color=colors)\n\nax.set_xlabel('IoU Score', fontsize=12, fontweight='bold')\nax.set_ylabel('Class', fontsize=12, fontweight='bold')\nax.set_title('Per-Class IoU Performance (U-Net)', fontsize=14, fontweight='bold')\nax.axvline(x=np.nanmean(mean_ious), color='red', linestyle='--', linewidth=2, label=f'Mean IoU: {np.nanmean(mean_ious):.4f}')\nax.legend(fontsize=11)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/unet_class_iou_barplot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Per-class IoU plot saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_image(model, img_path, device, img_size=(256, 256), return_overlay=False):\n    \"\"\"Predict segmentation mask for a single image\"\"\"\n    model.eval()\n    \n    # Load and preprocess image\n    img = Image.open(img_path).convert('RGB')\n    original_size = img.size\n    \n    img_resized = img.resize(img_size, Image.BILINEAR)\n    img_tensor = transforms.ToTensor()(img_resized)\n    img_tensor = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )(img_tensor)\n    img_tensor = img_tensor.unsqueeze(0).to(device)\n    \n    # Predict\n    with torch.no_grad():\n        output = model(img_tensor)\n        pred = output.argmax(dim=1).squeeze(0).cpu().numpy()\n    \n    # Convert to RGB\n    pred_rgb = class_index_to_rgb(pred)\n    pred_rgb_pil = Image.fromarray(pred_rgb).resize(original_size, Image.NEAREST)\n    \n    # Create overlay if requested\n    if return_overlay:\n        img_np = np.array(img)\n        pred_rgb_resized = np.array(Image.fromarray(pred_rgb).resize(original_size, Image.NEAREST))\n        overlay = (img_np * 0.6 + pred_rgb_resized * 0.4).astype(np.uint8)\n        return pred_rgb_pil, pred, Image.fromarray(overlay)\n    \n    return pred_rgb_pil, pred\n\n# Test on random validation images\nprint(\"\\nTesting inference on random validation samples:\\n\")\n\nnum_test_samples = 4\ntest_ids = random.sample(val_ids, min(num_test_samples, len(val_ids)))\n\nfig, axes = plt.subplots(num_test_samples, 4, figsize=(20, num_test_samples*5))\nif num_test_samples == 1:\n    axes = axes.reshape(1, -1)\n\nfor idx, test_id in enumerate(test_ids):\n    test_img_path = ROOT / 'JPEGImages' / f'{test_id}.jpg'\n    test_mask_path = ROOT / 'SegmentationClass' / f'{test_id}.png'\n    \n    # Predict\n    pred_rgb, pred_mask, overlay = predict_image(model, test_img_path, device, img_size=IMG_SIZE, return_overlay=True)\n    \n    # Load ground truth\n    ground_truth = Image.open(test_mask_path).resize(IMG_SIZE, Image.NEAREST)\n    gt_arr = mask_to_class_index(ground_truth)\n    gt_rgb = class_index_to_rgb(gt_arr)\n    \n    # Calculate IoU for this sample\n    sample_iou = compute_iou(pred_mask, gt_arr)\n    mean_sample_iou = np.nanmean(sample_iou)\n    \n    # Plot\n    axes[idx, 0].imshow(Image.open(test_img_path))\n    axes[idx, 0].set_title(f'Original Image\\n{test_id}', fontsize=11, fontweight='bold')\n    axes[idx, 0].axis('off')\n    \n    axes[idx, 1].imshow(gt_rgb)\n    axes[idx, 1].set_title('Ground Truth', fontsize=11, fontweight='bold')\n    axes[idx, 1].axis('off')\n    \n    axes[idx, 2].imshow(class_index_to_rgb(pred_mask))\n    axes[idx, 2].set_title(f'Prediction\\nmIoU: {mean_sample_iou:.4f}', fontsize=11, fontweight='bold')\n    axes[idx, 2].axis('off')\n    \n    axes[idx, 3].imshow(overlay)\n    axes[idx, 3].set_title('Overlay (60% img + 40% pred)', fontsize=11, fontweight='bold')\n    axes[idx, 3].axis('off')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/unet_inference_samples.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Inference samples saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nComputing confusion matrix (this may take a minute)...\")\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nall_preds = []\nall_targets = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, masks in val_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n        \n        # Flatten and filter out ignore index\n        for pred, mask in zip(preds, masks):\n            pred_np = pred.cpu().numpy().flatten()\n            mask_np = mask.cpu().numpy().flatten()\n            \n            valid_mask = mask_np != 255\n            all_preds.extend(pred_np[valid_mask])\n            all_targets.extend(mask_np[valid_mask])\n\n# Sample for faster computation (use all data for accurate matrix)\nsample_size = min(100000, len(all_preds))\nindices = np.random.choice(len(all_preds), sample_size, replace=False)\nsampled_preds = np.array(all_preds)[indices]\nsampled_targets = np.array(all_targets)[indices]\n\n# Compute confusion matrix\ncm = confusion_matrix(sampled_targets, sampled_preds, labels=list(range(21)))\ncm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)\n\n# Plot\nfig, ax = plt.subplots(figsize=(16, 14))\nsns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', \n            xticklabels=VOC_CLASSES, yticklabels=VOC_CLASSES,\n            cbar_kws={'label': 'Normalized Count'}, ax=ax)\nax.set_xlabel('Predicted Class', fontsize=12, fontweight='bold')\nax.set_ylabel('True Class', fontsize=12, fontweight='bold')\nax.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig('/kaggle/working/unet_confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Confusion matrix saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def export_predictions(model, val_loader, output_dir='/kaggle/working/predictions'):\n    \"\"\"Export all validation predictions as PNG files\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True)\n    \n    model.eval()\n    count = 0\n    \n    print(f\"Exporting predictions to {output_dir}...\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(val_loader):\n            images = images.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1)\n            \n            for i in range(preds.shape[0]):\n                pred_mask = preds[i].cpu().numpy()\n                pred_rgb = class_index_to_rgb(pred_mask)\n                pred_img = Image.fromarray(pred_rgb)\n                pred_img.save(output_dir / f'pred_{count:04d}.png')\n                count += 1\n            \n            if (batch_idx + 1) % 10 == 0:\n                print(f\"  Exported {count} predictions...\")\n    \n    print(f\"✓ Exported {count} predictions successfully!\")\n    return count\n\n# Uncomment to export all predictions\n# export_predictions(model, val_loader)\n\nprint(\"\\n🎉 All cells executed successfully! Your U-Net model is ready to use.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}